{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition on Kaggle\n",
    "\n",
    "Deploy and run face recognition service on Kaggle with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kaggle Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU in Kaggle:\n",
    "# Settings -> Accelerator -> GPU\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Kaggle paths\n",
    "KAGGLE_INPUT_PATH = '/kaggle/input'\n",
    "KAGGLE_WORKING_PATH = '/kaggle/working'\n",
    "\n",
    "print(f\"\\nKaggle Paths:\")\n",
    "print(f\"Input: {KAGGLE_INPUT_PATH}\")\n",
    "print(f\"Working: {KAGGLE_WORKING_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install system dependencies for OpenCV\n",
    "apt-get update && apt-get install -y \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglib2.0-0 \\\n",
    "    libsm6 \\\n",
    "    libxext6 \\\n",
    "    libxrender-dev \\\n",
    "    libgomp1\n",
    "\n",
    "# Install Python packages\n",
    "pip install -q \\\n",
    "    insightface==0.7.3 \\\n",
    "    onnxruntime-gpu==1.16.3 \\\n",
    "    faiss-gpu==1.7.4 \\\n",
    "    opencv-python-headless==4.9.0.80 \\\n",
    "    fastapi==0.109.0 \\\n",
    "    uvicorn==0.25.0 \\\n",
    "    python-multipart==0.0.6 \\\n",
    "    sqlalchemy==2.0.25 \\\n",
    "    asyncpg==0.29.0\n",
    "\n",
    "echo \"Dependencies installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Face Recognition Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from insightface.app import FaceAnalysis\n",
    "import faiss\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize face analysis app\n",
    "app = FaceAnalysis(\n",
    "    name='buffalo_l',\n",
    "    providers=['CUDAExecutionProvider', 'CPUExecutionProvider'],\n",
    "    allowed_modules=['detection', 'recognition']\n",
    ")\n",
    "app.prepare(ctx_id=0, det_thresh=0.5)\n",
    "\n",
    "print(\"✅ Face engine initialized with GPU support\")\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = 512\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "\n",
    "# ID mappings\n",
    "id_to_person = {}\n",
    "next_id = 0\n",
    "\n",
    "print(f\"✅ FAISS index created (dimension: {dimension})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load from Kaggle dataset\n",
    "# Add dataset to Kaggle: Add Data -> Search for face dataset\n",
    "\n",
    "# List available datasets\n",
    "import os\n",
    "datasets = os.listdir(KAGGLE_INPUT_PATH)\n",
    "print(\"Available datasets:\")\n",
    "for ds in datasets:\n",
    "    print(f\"  - {ds}\")\n",
    "\n",
    "# Example dataset path (adjust to your dataset)\n",
    "# DATASET_PATH = f\"{KAGGLE_INPUT_PATH}/lfw-dataset/lfw-deepfunneled\"\n",
    "\n",
    "# For demo, create sample data\n",
    "def load_sample_images():\n",
    "    \"\"\"Load sample images from Kaggle dataset or create dummy data\"\"\"\n",
    "    images = {}\n",
    "    \n",
    "    # Try to load from a dataset if available\n",
    "    if datasets:\n",
    "        dataset_path = Path(KAGGLE_INPUT_PATH) / datasets[0]\n",
    "        \n",
    "        # Find image files\n",
    "        for img_path in dataset_path.rglob(\"*.jpg\"):\n",
    "            person_id = img_path.parent.name\n",
    "            if person_id not in images:\n",
    "                images[person_id] = []\n",
    "            images[person_id].append(str(img_path))\n",
    "            \n",
    "            if len(images) >= 10:  # Limit for demo\n",
    "                break\n",
    "    \n",
    "    return images\n",
    "\n",
    "face_images = load_sample_images()\n",
    "print(f\"\\nLoaded {len(face_images)} persons with images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Face Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_enroll(person_id: str, image_paths: list):\n",
    "    \"\"\"Process and enroll faces for a person\"\"\"\n",
    "    global next_id\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for img_path in image_paths[:3]:  # Limit images per person\n",
    "        try:\n",
    "            # Read image\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Detect and extract faces\n",
    "            faces = app.get(img)\n",
    "            \n",
    "            if faces:\n",
    "                # Use first face\n",
    "                face = faces[0]\n",
    "                embedding = face.normed_embedding\n",
    "                embeddings.append(embedding)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    # Add to index\n",
    "    if embeddings:\n",
    "        embeddings_array = np.array(embeddings).astype('float32')\n",
    "        \n",
    "        # Add to FAISS\n",
    "        ids = list(range(next_id, next_id + len(embeddings)))\n",
    "        index.add(embeddings_array)\n",
    "        \n",
    "        # Update mappings\n",
    "        for idx in ids:\n",
    "            id_to_person[idx] = person_id\n",
    "        \n",
    "        next_id += len(embeddings)\n",
    "        \n",
    "        return len(embeddings)\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# Enroll all faces\n",
    "print(\"Enrolling faces...\")\n",
    "total_enrolled = 0\n",
    "\n",
    "for person_id, images in face_images.items():\n",
    "    enrolled = process_and_enroll(person_id, images)\n",
    "    total_enrolled += enrolled\n",
    "    print(f\"  {person_id}: {enrolled} faces enrolled\")\n",
    "\n",
    "print(f\"\\n✅ Total faces enrolled: {total_enrolled}\")\n",
    "print(f\"Index size: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Index to Kaggle Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index\n",
    "index_path = f\"{KAGGLE_WORKING_PATH}/face_index.faiss\"\n",
    "faiss.write_index(index, index_path)\n",
    "print(f\"✅ FAISS index saved to {index_path}\")\n",
    "\n",
    "# Save ID mappings\n",
    "mappings_path = f\"{KAGGLE_WORKING_PATH}/id_mappings.pkl\"\n",
    "with open(mappings_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'id_to_person': id_to_person,\n",
    "        'next_id': next_id\n",
    "    }, f)\n",
    "print(f\"✅ ID mappings saved to {mappings_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config_path = f\"{KAGGLE_WORKING_PATH}/config.json\"\n",
    "config = {\n",
    "    'dimension': dimension,\n",
    "    'index_size': index.ntotal,\n",
    "    'num_persons': len(set(id_to_person.values())),\n",
    "    'similarity_threshold': 0.65,\n",
    "    'model': 'buffalo_l',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"✅ Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Face Identification Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_face(image_path: str, top_k: int = 5, threshold: float = 0.65):\n",
    "    \"\"\"Identify a face from an image\"\"\"\n",
    "    \n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return {\"error\": \"Failed to read image\"}\n",
    "    \n",
    "    # Detect face\n",
    "    faces = app.get(img)\n",
    "    if not faces:\n",
    "        return {\"error\": \"No face detected\"}\n",
    "    \n",
    "    # Get embedding\n",
    "    face = faces[0]\n",
    "    embedding = face.normed_embedding.reshape(1, -1).astype('float32')\n",
    "    \n",
    "    # Search in index\n",
    "    if index.ntotal == 0:\n",
    "        return {\"error\": \"Index is empty\"}\n",
    "    \n",
    "    k = min(top_k, index.ntotal)\n",
    "    distances, indices = index.search(embedding, k)\n",
    "    \n",
    "    # Process results\n",
    "    matches = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if dist >= threshold:\n",
    "            person_id = id_to_person.get(int(idx), \"unknown\")\n",
    "            matches.append({\n",
    "                \"person_id\": person_id,\n",
    "                \"similarity\": float(dist),\n",
    "                \"index_id\": int(idx)\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"matches\": matches,\n",
    "        \"face_detected\": True,\n",
    "        \"bbox\": face.bbox.tolist(),\n",
    "        \"det_score\": float(face.det_score)\n",
    "    }\n",
    "\n",
    "# Test identification\n",
    "if face_images:\n",
    "    # Get a test image\n",
    "    test_person = list(face_images.keys())[0]\n",
    "    test_image = face_images[test_person][0]\n",
    "    \n",
    "    print(f\"Testing with image from {test_person}\")\n",
    "    result = identify_face(test_image)\n",
    "    \n",
    "    print(\"\\nIdentification Result:\")\n",
    "    print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Use Kaggle Secrets for Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access Kaggle Secrets\n",
    "# Add secrets in Kaggle: Settings -> Secrets -> Add Secret\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Initialize secrets client\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "# Example: Get API keys or database credentials\n",
    "# api_key = secrets.get_secret(\"API_KEY\")\n",
    "# db_password = secrets.get_secret(\"DB_PASSWORD\")\n",
    "\n",
    "# For demo, set dummy values\n",
    "api_key = \"demo_key\"\n",
    "similarity_threshold = 0.65\n",
    "\n",
    "print(\"Configuration loaded from secrets\")\n",
    "print(f\"Similarity threshold: {similarity_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Simple API Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple FastAPI server\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "# Create API\n",
    "api = FastAPI(title=\"Face Recognition on Kaggle\")\n",
    "\n",
    "@api.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"service\": \"Face Recognition\",\n",
    "        \"platform\": \"Kaggle\",\n",
    "        \"gpu\": torch.cuda.is_available(),\n",
    "        \"index_size\": index.ntotal\n",
    "    }\n",
    "\n",
    "@api.post(\"/identify\")\n",
    "async def identify(image: UploadFile = File(...)):\n",
    "    # Save uploaded image\n",
    "    temp_path = f\"{KAGGLE_WORKING_PATH}/temp_upload.jpg\"\n",
    "    \n",
    "    contents = await image.read()\n",
    "    with open(temp_path, 'wb') as f:\n",
    "        f.write(contents)\n",
    "    \n",
    "    # Identify face\n",
    "    result = identify_face(temp_path)\n",
    "    \n",
    "    return JSONResponse(content=result)\n",
    "\n",
    "@api.get(\"/stats\")\n",
    "async def stats():\n",
    "    return {\n",
    "        \"total_faces\": index.ntotal,\n",
    "        \"total_persons\": len(set(id_to_person.values())),\n",
    "        \"dimension\": dimension,\n",
    "        \"threshold\": similarity_threshold\n",
    "    }\n",
    "\n",
    "# Run server in background (for demo)\n",
    "def run_server():\n",
    "    uvicorn.run(api, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# Note: In Kaggle notebook, the server won't be accessible externally\n",
    "# This is for demonstration purposes\n",
    "print(\"API server ready (internal only)\")\n",
    "print(\"Endpoints:\")\n",
    "print(\"  GET  /\")\n",
    "print(\"  POST /identify\")\n",
    "print(\"  GET  /stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Model for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package everything for download\n",
    "import zipfile\n",
    "\n",
    "output_zip = f\"{KAGGLE_WORKING_PATH}/face_recognition_model.zip\"\n",
    "\n",
    "with zipfile.ZipFile(output_zip, 'w') as zipf:\n",
    "    # Add index\n",
    "    zipf.write(index_path, 'face_index.faiss')\n",
    "    \n",
    "    # Add mappings\n",
    "    zipf.write(mappings_path, 'id_mappings.pkl')\n",
    "    \n",
    "    # Add config\n",
    "    zipf.write(config_path, 'config.json')\n",
    "\n",
    "print(f\"✅ Model package created: {output_zip}\")\n",
    "print(f\"Size: {os.path.getsize(output_zip) / 1024 / 1024:.2f} MB\")\n",
    "print(\"\\nYou can download this file from the output directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}