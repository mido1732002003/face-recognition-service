{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Threshold Calibration\n",
    "\n",
    "This notebook helps calibrate the similarity threshold for face recognition using ROC and DET curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Validation Dataset\n",
    "\n",
    "Prepare genuine (same person) and impostor (different person) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load or generate similarity scores\n",
    "# In practice, compute these from your validation set\n",
    "\n",
    "# Simulate genuine pairs (same person)\n",
    "genuine_scores = np.random.beta(8, 2, 1000)  # Higher scores\n",
    "genuine_labels = np.ones(len(genuine_scores))\n",
    "\n",
    "# Simulate impostor pairs (different persons)\n",
    "impostor_scores = np.random.beta(2, 8, 5000)  # Lower scores\n",
    "impostor_labels = np.zeros(len(impostor_scores))\n",
    "\n",
    "# Combine\n",
    "all_scores = np.concatenate([genuine_scores, impostor_scores])\n",
    "all_labels = np.concatenate([genuine_labels, impostor_labels])\n",
    "\n",
    "print(f\"Genuine pairs: {len(genuine_scores)}\")\n",
    "print(f\"Impostor pairs: {len(impostor_scores)}\")\n",
    "print(f\"Genuine score range: [{genuine_scores.min():.3f}, {genuine_scores.max():.3f}]\")\n",
    "print(f\"Impostor score range: [{impostor_scores.min():.3f}, {impostor_scores.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Score Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(genuine_scores, bins=50, alpha=0.7, label='Genuine', density=True, color='green')\n",
    "axes[0].hist(impostor_scores, bins=50, alpha=0.7, label='Impostor', density=True, color='red')\n",
    "axes[0].set_xlabel('Similarity Score')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Score Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "data_to_plot = [genuine_scores, impostor_scores]\n",
    "bp = axes[1].boxplot(data_to_plot, labels=['Genuine', 'Impostor'])\n",
    "axes[1].set_ylabel('Similarity Score')\n",
    "axes[1].set_title('Score Distribution (Box Plot)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"Genuine - Mean: {genuine_scores.mean():.3f}, Std: {genuine_scores.std():.3f}\")\n",
    "print(f\"Impostor - Mean: {impostor_scores.mean():.3f}, Std: {impostor_scores.std():.3f}\")\n",
    "print(f\"Score overlap: {(genuine_scores.min() < impostor_scores.max()):.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Find optimal threshold points\n",
    "eer_threshold_idx = np.argmin(np.abs(fpr - (1 - tpr)))  # Equal Error Rate\n",
    "eer_threshold = thresholds[eer_threshold_idx]\n",
    "eer = fpr[eer_threshold_idx]\n",
    "\n",
    "# FAR targets\n",
    "far_targets = [0.001, 0.01, 0.1]  # 0.1%, 1%, 10%\n",
    "far_thresholds = []\n",
    "\n",
    "for target_far in far_targets:\n",
    "    idx = np.argmin(np.abs(fpr - target_far))\n",
    "    far_thresholds.append({\n",
    "        'FAR': fpr[idx],\n",
    "        'TAR': tpr[idx],\n",
    "        'Threshold': thresholds[idx]\n",
    "    })\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')\n",
    "\n",
    "# Mark special points\n",
    "plt.scatter(fpr[eer_threshold_idx], tpr[eer_threshold_idx], color='red', s=100, \n",
    "            label=f'EER = {eer:.3f} @ {eer_threshold:.3f}')\n",
    "\n",
    "for far_point in far_thresholds:\n",
    "    plt.scatter(far_point['FAR'], far_point['TAR'], s=100, \n",
    "                label=f\"FAR={far_point['FAR']:.3f} @ {far_point['Threshold']:.3f}\")\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Accept Rate (FAR)')\n",
    "plt.ylabel('True Accept Rate (TAR)')\n",
    "plt.title('ROC Curve - Face Recognition')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print recommendations\n",
    "print(\"\\nRecommended Thresholds:\")\n",
    "print(f\"EER Point: {eer_threshold:.3f} (Balanced approach)\")\n",
    "for i, far_point in enumerate(far_thresholds):\n",
    "    use_case = ['High Security', 'Standard', 'Convenience'][i]\n",
    "    print(f\"FAR {far_point['FAR']:.3f}: {far_point['Threshold']:.3f} ({use_case})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DET Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Compute FRR (False Reject Rate)\n",
    "frr = 1 - tpr\n",
    "\n",
    "# Convert to DET scale (probit)\n",
    "def probit(p):\n",
    "    \"\"\"Convert probability to probit scale\"\"\"\n",
    "    p = np.clip(p, 1e-7, 1 - 1e-7)  # Avoid infinities\n",
    "    return norm.ppf(p)\n",
    "\n",
    "# Plot DET curve\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot in probit scale\n",
    "ax.plot(probit(fpr), probit(frr), 'b-', linewidth=2)\n",
    "\n",
    "# Mark EER point\n",
    "ax.scatter(probit(fpr[eer_threshold_idx]), probit(frr[eer_threshold_idx]), \n",
    "           color='red', s=100, zorder=5, label=f'EER = {eer:.3f}')\n",
    "\n",
    "# Mark FAR points\n",
    "for far_point in far_thresholds:\n",
    "    idx = np.argmin(np.abs(fpr - far_point['FAR']))\n",
    "    ax.scatter(probit(fpr[idx]), probit(frr[idx]), s=100, zorder=5,\n",
    "              label=f\"FAR = {far_point['FAR']:.3f}\")\n",
    "\n",
    "# Set ticks in probability scale\n",
    "ticks = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "tick_labels = ['0.1%', '1%', '5%', '10%', '20%', '50%']\n",
    "\n",
    "ax.set_xticks([probit(t) for t in ticks])\n",
    "ax.set_xticklabels(tick_labels)\n",
    "ax.set_yticks([probit(t) for t in ticks])\n",
    "ax.set_yticklabels(tick_labels)\n",
    "\n",
    "ax.set_xlabel('False Accept Rate (FAR)')\n",
    "ax.set_ylabel('False Reject Rate (FRR)')\n",
    "ax.set_title('DET Curve - Face Recognition')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Threshold Selection Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold(threshold, scores, labels):\n",
    "    \"\"\"Evaluate performance at specific threshold\"\"\"\n",
    "    predictions = scores >= threshold\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
    "    \n",
    "    far = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    frr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    tar = 1 - frr\n",
    "    accuracy = (tp + tn) / len(labels)\n",
    "    \n",
    "    return {\n",
    "        'Threshold': threshold,\n",
    "        'FAR': far,\n",
    "        'FRR': frr,\n",
    "        'TAR': tar,\n",
    "        'Accuracy': accuracy,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    }\n",
    "\n",
    "# Interactive threshold selection\n",
    "test_thresholds = [0.5, 0.6, 0.65, 0.7, 0.75, 0.8]\n",
    "results = []\n",
    "\n",
    "for thresh in test_thresholds:\n",
    "    result = evaluate_threshold(thresh, all_scores, all_labels)\n",
    "    results.append(result)\n",
    "\n",
    "# Display results table\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['F1'] = 2 * df_results['TP'] / (2 * df_results['TP'] + df_results['FP'] + df_results['FN'])\n",
    "\n",
    "print(\"\\nThreshold Analysis Table:\")\n",
    "print(df_results[['Threshold', 'FAR', 'FRR', 'TAR', 'Accuracy', 'F1']].round(4).to_string(index=False))\n",
    "\n",
    "# Visualize trade-offs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# FAR vs FRR\n",
    "axes[0].plot(df_results['Threshold'], df_results['FAR'], 'r-o', label='FAR')\n",
    "axes[0].plot(df_results['Threshold'], df_results['FRR'], 'b-o', label='FRR')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Error Rate')\n",
    "axes[0].set_title('FAR vs FRR Trade-off')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy and F1\n",
    "axes[1].plot(df_results['Threshold'], df_results['Accuracy'], 'g-o', label='Accuracy')\n",
    "axes[1].plot(df_results['Threshold'], df_results['F1'], 'm-o', label='F1 Score')\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Accuracy and F1 Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final threshold\n",
    "selected_threshold = 0.65  # Adjust based on your requirements\n",
    "\n",
    "final_metrics = evaluate_threshold(selected_threshold, all_scores, all_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL THRESHOLD CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nSelected Threshold: {selected_threshold}\")\n",
    "print(f\"\\nExpected Performance:\")\n",
    "print(f\"  - False Accept Rate (FAR): {final_metrics['FAR']:.2%}\")\n",
    "print(f\"  - False Reject Rate (FRR): {final_metrics['FRR']:.2%}\")\n",
    "print(f\"  - True Accept Rate (TAR):  {final_metrics['TAR']:.2%}\")\n",
    "print(f\"  - Overall Accuracy:        {final_metrics['Accuracy']:.2%}\")\n",
    "\n",
    "print(f\"\\n📝 Update your .env file:\")\n",
    "print(f\"   SIMILARITY_THRESHOLD={selected_threshold}\")\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    'threshold': selected_threshold,\n",
    "    'expected_far': final_metrics['FAR'],\n",
    "    'expected_frr': final_metrics['FRR'],\n",
    "    'validation_auc': roc_auc,\n",
    "    'validation_samples': len(all_labels)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('threshold_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Configuration saved to threshold_config.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}